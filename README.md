# Evaluating Language Models for Harmful Prompt Detection

## Project Overview
This project involves using a dataset from the LLM-EvaluationHub to evaluate and compare the effectiveness of different language models in detecting harmful prompts. Participants are expected to implement at least two models, perform data visualization, and analyze which model best identifies harmful content.

## Objectives

Data Analysis: Use the provided dataset for exploratory analysis and visualization.

Model Implementation: Evaluate a minimum of two language models for harmful prompt detection (the more models you use, the higher marks you get).

Performance Comparison: Compare models based on their accuracy and effectiveness in identifying harmful prompts.
